<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Model Selection</title>
    <meta charset="utf-8" />
    <meta name="author" content="Zhang Yixue" />
    <script src="site_libs/header-attrs/header-attrs.js"></script>
    <link href="site_libs/remark-css/default.css" rel="stylesheet" />
    <link href="site_libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="site_libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="site_libs/tile-view/tile-view.js"></script>
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"xffa7dfc650a4a99ac7702bef5e237cb","expires":1}</script>
    <script src="site_libs/himalaya/himalaya.js"></script>
    <script src="site_libs/js-cookie/js.cookie.js"></script>
    <link href="site_libs/editable/editable.css" rel="stylesheet" />
    <script src="site_libs/editable/editable.js"></script>
    <script src="site_libs/fabric/fabric.min.js"></script>
    <link href="site_libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="site_libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <link href="site_libs/panelset/panelset.css" rel="stylesheet" />
    <script src="site_libs/panelset/panelset.js"></script>
    <script src="site_libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <script src="site_libs/htmlwidgets/htmlwidgets.js"></script>
    <link href="site_libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="site_libs/datatables-binding/datatables.js"></script>
    <script src="site_libs/jquery/jquery-3.6.0.min.js"></script>
    <link href="site_libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="site_libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="site_libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="site_libs/crosstalk/css/crosstalk.min.css" rel="stylesheet" />
    <script src="site_libs/crosstalk/js/crosstalk.min.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Model Selection
### Zhang Yixue
### June , 2022

---


&lt;style type="text/css"&gt;
&lt;style type="text/css"&gt;
body, td {
   font-size: 20px;
}
code.r{
  font-size: 19px;
}
pre {
  font-size: 70px
}
&lt;/style&gt;
&lt;/style&gt;

















---
class: center, middle
# Overcomplicated models 
# Oversimplified models
???
Since kyle's course content is too much, so I just focus on one question to start my presentation ,In this process I will also talk generally about what is Generalized linear model and linear mixed model but there would not be too much details because of the litmiithed time. Anyway, the question is about when we try to Analysis our data, even we have built  a model to explain the relationship between the response variables and independent variables, how could we know this model is good or not? Based this question, I rebuilt two kinds of situation, one is Overcomplicated models, another is  Oversimplified models, in the fist case ,i would use the Generalized linear model as the example , in the second case i would use the linear mixed model as the example.
---
class: center, middle
# Overcomplicated models
---
## example 1 
<div id="htmlwidget-8d32cfd9db97395c4a5f" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-8d32cfd9db97395c4a5f">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400"],["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400"],[0,1,1,1,0,1,1,0,1,0,0,0,1,0,1,0,0,0,0,1,0,1,0,0,1,1,1,1,1,0,0,0,0,1,0,0,0,0,1,1,0,1,1,0,0,1,1,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,1,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,1,1,0,1,1,0,1,0,0,0,0,0,0,1,1,0,1,0,1,0,0,1,0,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,0,0,0,0,1,1,1,0,0,1,1,0,1,0,1,0,0,1,0,1,1,1,0,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,0,1,0,1,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,1,1,0,0,1,0,1,1,0,0,1,0,0,0,0,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,1,0,1,0,0,0,1,1,1,1,1,0,0,0,0,0],[380,660,800,640,520,760,560,400,540,700,800,440,760,700,700,480,780,360,800,540,500,660,600,680,760,800,620,520,780,520,540,760,600,800,360,400,580,520,500,520,560,580,600,500,700,460,580,500,440,400,640,440,740,680,660,740,560,380,400,600,620,560,640,680,580,600,740,620,580,800,640,300,480,580,720,720,560,800,540,620,700,620,500,380,500,520,600,600,700,660,700,720,800,580,660,660,640,480,700,400,340,580,380,540,660,740,700,480,400,480,680,420,360,600,720,620,440,700,800,340,520,480,520,500,720,540,600,740,540,460,620,640,580,500,560,500,560,700,620,600,640,700,620,580,580,380,480,560,480,740,800,400,640,580,620,580,560,480,660,700,600,640,700,520,580,700,440,720,500,600,400,540,680,800,500,620,520,620,620,300,620,500,700,540,500,800,560,580,560,500,640,800,640,380,600,560,660,400,600,580,800,580,700,420,600,780,740,640,540,580,740,580,460,640,600,660,340,460,460,560,540,680,480,800,800,720,620,540,480,720,580,600,380,420,800,620,660,480,500,700,440,520,680,620,540,800,680,440,680,640,660,620,520,540,740,640,520,620,520,640,680,440,520,620,520,380,560,600,680,500,640,540,680,660,520,600,460,580,680,660,660,360,660,520,440,600,800,660,800,420,620,800,680,800,480,520,560,460,540,720,640,660,400,680,220,580,540,580,540,440,560,660,660,520,540,300,340,780,480,540,460,460,500,420,520,680,680,560,580,500,740,660,420,560,460,620,520,620,540,660,500,560,500,580,520,500,600,580,400,620,780,620,580,700,540,760,700,720,560,720,520,540,680,460,560,480,460,620,580,800,540,680,680,620,560,560,620,800,640,540,700,540,540,660,480,420,740,580,640,640,800,660,600,620,460,620,560,460,700,600],[3.61,3.67,4,3.19,2.93,3,2.98,3.08,3.39,3.92,4,3.22,4,3.08,4,3.44,3.87,2.56,3.75,3.81,3.17,3.63,2.82,3.19,3.35,3.66,3.61,3.74,3.22,3.29,3.78,3.35,3.4,4,3.14,3.05,3.25,2.9,3.13,2.68,2.42,3.32,3.15,3.31,2.94,3.45,3.46,2.97,2.48,3.35,3.86,3.13,3.37,3.27,3.34,4,3.19,2.94,3.65,2.82,3.18,3.32,3.67,3.85,4,3.59,3.62,3.3,3.69,3.73,4,2.92,3.39,4,3.45,4,3.36,4,3.12,4,2.9,3.07,2.71,2.91,3.6,2.98,3.32,3.48,3.28,4,3.83,3.64,3.9,2.93,3.44,3.33,3.52,3.57,2.88,3.31,3.15,3.57,3.33,3.94,3.95,2.97,3.56,3.13,2.93,3.45,3.08,3.41,3,3.22,3.84,3.99,3.45,3.72,3.7,2.92,3.74,2.67,2.85,2.98,3.88,3.38,3.54,3.74,3.19,3.15,3.17,2.79,3.4,3.08,2.95,3.57,3.33,4,3.4,3.58,3.93,3.52,3.94,3.4,3.4,3.43,3.4,2.71,2.91,3.31,3.74,3.38,3.94,3.46,3.69,2.86,2.52,3.58,3.49,3.82,3.13,3.5,3.56,2.73,3.3,4,3.24,3.77,4,3.62,3.51,2.81,3.48,3.43,3.53,3.37,2.62,3.23,3.33,3.01,3.78,3.88,4,3.84,2.79,3.6,3.61,2.88,3.07,3.35,2.94,3.54,3.76,3.59,3.47,3.59,3.07,3.23,3.63,3.77,3.31,3.2,4,3.92,3.89,3.8,3.54,3.63,3.16,3.5,3.34,3.02,2.87,3.38,3.56,2.91,2.9,3.64,2.98,3.59,3.28,3.99,3.02,3.47,2.9,3.5,3.58,3.02,3.43,3.42,3.29,3.28,3.38,2.67,3.53,3.05,3.49,4,2.86,3.45,2.76,3.81,2.96,3.22,3.04,3.91,3.34,3.17,3.64,3.73,3.31,3.21,4,3.55,3.52,3.35,3.3,3.95,3.51,3.81,3.11,3.15,3.19,3.95,3.9,3.34,3.24,3.64,3.46,2.81,3.95,3.33,3.67,3.32,3.12,2.98,3.77,3.58,3,3.14,3.94,3.27,3.45,3.1,3.39,3.31,3.22,3.7,3.15,2.26,3.45,2.78,3.7,3.97,2.55,3.25,3.16,3.07,3.5,3.4,3.3,3.6,3.15,3.98,2.83,3.46,3.17,3.51,3.13,2.98,4,3.67,3.77,3.65,3.46,2.84,3,3.63,3.71,3.28,3.14,3.58,3.01,2.69,2.7,3.9,3.31,3.48,3.34,2.93,4,3.59,2.96,3.43,3.64,3.71,3.15,3.09,3.2,3.47,3.23,2.65,3.95,3.06,3.35,3.03,3.35,3.8,3.36,2.85,4,3.43,3.12,3.52,3.78,2.81,3.27,3.31,3.69,3.94,4,3.49,3.14,3.44,3.36,2.78,2.93,3.63,4,3.89,3.77,3.76,2.42,3.37,3.78,3.49,3.63,4,3.12,2.7,3.65,3.49,3.51,4,2.62,3.02,3.86,3.36,3.17,3.51,3.05,3.88,3.38,3.75,3.99,4,3.04,2.63,3.65,3.89],[3,3,1,4,4,2,1,2,3,2,4,1,1,2,1,3,4,3,2,1,3,2,4,4,2,1,1,4,2,1,4,3,3,3,1,2,1,3,2,3,2,2,2,3,2,3,2,4,4,3,3,4,4,2,3,3,3,3,2,4,2,4,3,3,3,2,4,1,1,1,3,4,4,2,4,3,3,3,1,1,4,2,2,4,3,2,2,2,1,2,2,1,2,2,2,2,4,2,2,3,3,3,4,3,2,2,1,2,3,2,4,4,3,1,3,3,2,2,1,3,2,2,3,3,3,4,1,4,2,4,2,2,2,3,2,3,4,3,2,1,2,4,4,3,4,3,2,3,1,1,1,2,2,3,3,4,2,1,2,3,2,2,2,2,2,1,4,3,3,3,3,3,3,2,4,2,2,3,3,3,3,4,2,2,4,2,3,2,2,2,2,3,3,4,2,2,3,4,3,4,3,2,1,4,1,3,1,1,3,2,4,2,2,3,2,3,1,1,1,2,3,3,1,3,2,3,2,4,2,2,4,3,2,3,1,2,2,2,4,3,2,1,3,2,1,3,2,2,3,3,4,4,2,4,4,3,2,3,2,2,2,2,3,3,3,3,4,3,2,3,2,3,2,1,2,2,3,1,4,2,2,3,4,4,2,4,1,4,4,4,2,2,2,1,1,3,1,2,2,3,2,3,2,2,3,4,1,2,2,3,3,2,3,4,4,2,2,4,4,1,3,2,4,2,3,1,2,2,2,4,3,3,1,3,3,1,3,4,1,3,4,3,4,2,3,3,2,2,2,2,2,3,3,2,2,1,2,1,3,3,1,1,2,2,1,3,3,3,1,2,2,3,1,1,2,4,2,2,3,2,2,2,2,1,2,1,2,2,2,2,2,2,3,2,3,2,3,2,2,3]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>row.names<\/th>\n      <th>admit<\/th>\n      <th>gre<\/th>\n      <th>gpa<\/th>\n      <th>rank<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[2,3,4,5]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
???
here is the example 1, the response variable is about Admission Qualifications, 1 means you got that ,0 means you faild, and here are 3 independent variables, gre is the grades of one kind of tests, and gpa is the average of all grades about the courses in your  graduate school. rank is the  the ranking of the school you are applying to. obviously, the response variable does not fit the normal distribution, and it is binomial data!! so we need to use the glm!!
---
## Maximum Likelihood Estimation
- MLE is the most likely value of the parameters given the data.
- Deviance is difference between 
likelihoods and the saturated Lik
.pull-left[
&lt;img src="../data/img/sta-slides1.png"
width="400" /&gt;
]

.pull-right[
&lt;img src="../data/img/sta-slides2.png"
width="350" /&gt;
]

???
- 左 before showing the code , I have to introduce what is Maximum Likelihood Estimation generally. Briefly MLE is the most likely value of the parameters given the data which means that we rebuit another function and put all the data and parameters in this new function, then we just need to find the turing point of this function, just like  seeking  the derivative.
- 右 In addition to finding the parameters of glm, by this way, we also use this method to evaluate the  fitted model, first we need to build the Null model which means minimal information explained and the formula looks like y ~ 1 (no predictors); then  the Saturated model which means maximal information explained y~ b1x1 + b2x2 + x3 +….+ xn(one parameter for each data );  lastyly, the Fitted model is  our prediction model,the fomular looks like:y ~ b1x1 + b2x2, we use this way to evaluate, I would explain more in the code

---
.panelset[
.panel[.panel-name[Code-built]

```r
mydata &lt;- read.csv("E:/academic_resources/note-tutorial/data/binary.csv",header=T,sep=" ")
mydata$rank &lt;- as.factor(mydata$rank)
#全模型
dim(mydata)
mydata$x1 &lt;- as.factor(1:nrow(mydata))
glmS &lt;- glm(admit ~ x1, data = mydata, family = "binomial")
#零模型
glm0 &lt;- glm(admit ~ 1, data = mydata, family = "binomial")
#被测试模型1
glm1 &lt;- glm(admit ~ gre+gpa+rank, data = mydata, family = "binomial")
#被测试模型2
glm2 &lt;- glm(admit ~ gpa+rank, data = mydata, family = "binomial")
```
]

.panel[.panel-name[Code-compare]

```r
#Q1:glmS and glm0 ?
summary(glmS)$deviance
summary(glm0)$deviance
summary(glm1)$deviance
2*(logLik(glmS)-logLik(glm0))
2*(logLik(glmS)-logLik(glm1))
summary(glm1)

#Q2:glm1 and glm2 ?
summary(glm1)$deviance
summary(glm2)$deviance
anova(glm1,glm2,test='Chisq')

#Q3:another way-AIC
library(MuMIn)
options(na.action = "na.fail") 
dredge(glm1)
print(AICglm1 &lt;- -2*(logLik(glm1))+ 2*6)
print(AICglm2 &lt;- -2*(logLik(glm2))+ 2*5)
```
]

.panel[.panel-name[Q1]
.left-column[

```
## [1] 2.320633e-09
```

```
## [1] 499.9765
```

```
## [1] 458.5175
```

```
## 'log Lik.' 499.9765 (df=400)
```

```
## 'log Lik.' 458.5175 (df=400)
```
]

.right-column[

&lt;img src="E:/academic_resources/Tutorials/data/img/sta-slides3.png"
width="800" /&gt;
]
]

.panel[.panel-name[Q2]

```
## [1] 458.5175
```

```
## [1] 462.8753
```

```
## Analysis of Deviance Table
## 
## Model 1: admit ~ gre + gpa + rank
## Model 2: admit ~ gpa + rank
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  
## 1       394     458.52                       
## 2       395     462.88 -1  -4.3578  0.03684 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]



.panel[.panel-name[Q3]


```
## 'log Lik.' 470.5175 (df=6)
```

```
## 'log Lik.' 472.8753 (df=5)
```

```
## Global model call: glm(formula = admit ~ gre + gpa + rank, family = "binomial", 
##     data = mydata)
## ---
## Model selection table 
##   (Intrc)    gpa      gre rank df   logLik  AICc delta weight
## 8 -3.9900 0.8040 0.002264    +  6 -229.259 470.7  0.00  0.686
## 6 -3.4640 1.0520             +  5 -231.438 473.0  2.30  0.218
## 7 -1.8020        0.003224    +  5 -232.266 474.7  3.95  0.095
## 5  0.1643                    +  4 -237.483 483.1 12.34  0.001
## 4 -4.9490 0.7547 0.002691       3 -240.172 486.4 15.67  0.000
## 3 -2.9010        0.003582       2 -243.028 490.1 19.36  0.000
## 2 -4.3580 1.0510                2 -243.484 491.0 20.27  0.000
## 1 -0.7653                       1 -249.988 502.0 31.26  0.000
## Models ranked by AICc(x)
```
]
]

???
- 1. first, we put our data into the matrix called mydata,  and because the rank is Categorical variable, so we set it as the factor. then,in order to buid the satured model we create a new vector we called x1 , and x1 is made of the number from 1 to 400 because there are 400 rows in the raw data, but i am not sure why buiding  the saturated model in this way, maybe  due to the assumption that consecutive natural numbers are perfectly normally distributed. second we build the null model, as you can see there is no predictor, and we build two fitted model, one includes three predictors anthoer includes 2 predictors , there is no reason why i remove the gre, doing this just for comparison and Explanation.
-2. Q1: 
    - as we said before, the deviance is the difference between the likelihood of model we intersted and the likelihhod of saturated, 
    - so the result of summary(glmS)$deviance should touch 0, because there is no differnce between the saturated model. 
    - and the result of summary(glm0)$deviance (499)should be the maximal because do you remeber the picture, the distance between the null model and the saturated model is the largest. we could see it from the result of summary function of any other fitted model(here!!!) 
    - and the result of summary(glm1)$deviance(458) is the difference between saturated model and glm1 model, we could see it in the summary result(here!!!!)
2. so the following two fomular just for showing how to caculte the deviance , their results should be the same as these two lines of code.lets see the result. 
-2. Q1:generally,  among the several fitted models,The smaller the residuals, the better the model!!! becase  when the deviance is smaller,it means our fitted model is closer to the saturated model, and the unexplained part of the response variation is less. so now we could see Q2
- C-c!!!!Q2：we use the code  like this to get these two fitted model to get their devicane , and see the result(Q2)!!!!! the first one is smaller, could we say  that the glm1 is better????? No!!!because we need to test the diffence bwteween these two model's deviance is significant or not !!! C-c!!!we use anova function, and specify the chi-squared distribution to test, Q2!!see the result ,it significant, so the first one is better!!!
- C-C!!!!should i write all fitted model like this way ？C-c？？no！we could use the function called dredge from the MUMIN package and we would get all subset models and AIC,AIC is an Indicator to help us to know which model is the best, and these two lines show how to calculate AIC, as you can see the likelihood also used here ,so the we should choose the smaller AIC, let's see the result Q3!!! 
- Q3!! the first row is the glm1， every preditors are included,the secon row is glm2, just remove the gre.
 


---
class: center, middle
# Oversimplified models
???
 let's see situation 2 
---
## example 2
<div id="htmlwidget-d701554a25665d09d09f" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-d701554a25665d09d09f">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640","641","642","643","644","645","646","647","648","649","650","651","652","653","654","655","656","657","658","659","660","661","662","663","664","665","666","667","668","669","670","671","672","673","674","675","676","677","678","679","680","681","682","683","684","685","686","687","688","689","690","691","692","693","694","695","696","697","698","699","700","701","702","703","704","705","706","707","708","709","710","711","712","713","714","715","716","717","718","719","720","721","722","723","724","725","726","727","728","729","730","731","732","733","734","735","736","737","738","739","740","741","742","743","744","745","746","747","748","749","750","751","752","753","754","755","756","757","758","759","760","761","762","763","764","765","766","767","768","769","770","771","772","773","774","775","776","777","778","779","780","781","782","783","784","785","786","787","788","789","790","791","792","793","794","795","796","797","798","799","800","801","802","803","804","805","806","807","808","809","810","811","812","813","814","815","816","817","818","819","820","821","822","823","824","825","826","827","828","829","830","831","832","833","834","835","836","837","838","839","840","841","842","843","844","845","846","847","848","849","850","851","852","853","854","855","856","857","858","859","860","861","862","863","864","865","866","867","868","869","870","871","872","873","874","875","876","877","878","879","880","881","882","883","884","885","886","887","888","889","890","891","892","893","894","895","896","897","898","899","900","901","902","903","904","905","906","907","908","909","910","911","912","913","914","915","916","917","918","919","920","921","922","923","924","925","926","927","928","929","930","931","932","933","934","935","936","937","938","939","940","941","942","943","944","945","946","947","948","949","950","951","952","953","954","955","956","957","958","959","960","961","962","963","964","965","966","967","968","969","970","971","972","973","974","975","976","977","978","979","980","981","982","983","984","985","986","987","988","989","990","991","992","993","994","995","996","997","998","999","1000","1001","1002","1003","1004","1005","1006","1007","1008","1009","1010","1011","1012","1013","1014","1015","1016","1017","1018","1019","1020","1021","1022","1023","1024","1025","1026","1027","1028","1029","1030","1031","1032","1033","1034","1035","1036","1037","1038","1039","1040","1041","1042","1043","1044","1045","1046","1047","1048","1049","1050","1051","1052","1053","1054","1055","1056","1057","1058","1059","1060","1061","1062","1063","1064","1065","1066","1067","1068","1069","1070","1071","1072","1073","1074","1075","1076","1077","1078","1079","1080","1081","1082","1083","1084","1085","1086","1087","1088","1089","1090","1091","1092","1093","1094","1095","1096","1097","1098","1099","1100","1101","1102","1103","1104","1105","1106","1107","1108","1109","1110","1111","1112","1113","1114","1115","1116","1117","1118","1119","1120","1121","1122","1123","1124","1125","1126","1127","1128","1129","1130","1131","1132","1133","1134","1135","1136","1137","1138","1139","1140","1141","1142","1143","1144","1145","1146","1147","1148","1149","1150","1151","1152","1153","1154","1155","1156","1157","1158","1159","1160","1161","1162","1163","1164","1165","1166","1167","1168","1169","1170","1171","1172","1173","1174","1175","1176","1177","1178","1179","1180","1181","1182","1183","1184","1185","1186","1187","1188","1189","1190","1191","1192","1193","1194","1195","1196","1197","1198","1199","1200","1201","1202","1203","1204","1205","1206","1207","1208","1209","1210","1211","1212","1213","1214","1215","1216","1217","1218","1219","1220","1221","1222","1223","1224","1225","1226","1227","1228","1229","1230","1231","1232","1233","1234","1235","1236","1237","1238","1239","1240","1241","1242","1243","1244","1245","1246","1247","1248","1249","1250","1251","1252","1253","1254","1255","1256","1257","1258","1259","1260","1261","1262","1263","1264","1265","1266","1267","1268","1269","1270","1271","1272","1273","1274","1275","1276","1277","1278","1279","1280","1281","1282","1283","1284","1285","1286","1287","1288","1289","1290","1291","1292","1293","1294","1295","1296","1297","1298","1299","1300","1301","1302","1303","1304","1305","1306","1307","1308","1309","1310","1311","1312","1313","1314","1315","1316","1317","1318","1319","1320","1321","1322","1323","1324","1325","1326","1327","1328","1329","1330","1331","1332","1333","1334","1335","1336","1337","1338","1339","1340","1341","1342","1343","1344","1345","1346","1347","1348","1349","1350","1351","1352","1353","1354","1355","1356","1357","1358","1359","1360","1361","1362","1363","1364","1365","1366","1367","1368","1369","1370","1371","1372","1373","1374","1375","1376","1377","1378","1379","1380","1381","1382","1383","1384","1385","1386","1387","1388","1389","1390","1391","1392","1393","1394","1395","1396","1397","1398","1399","1400","1401","1402","1403","1404","1405","1406","1407","1408","1409","1410","1411","1412","1413","1414","1415","1416","1417","1418","1419","1420","1421","1422","1423","1424","1425","1426","1427","1428","1429","1430","1431","1432","1433","1434","1435","1436","1437","1438","1439","1440","1441","1442","1443","1444","1445","1446","1447","1448","1449","1450","1451","1452","1453","1454","1455","1456","1457","1458","1459","1460","1461","1462","1463","1464","1465","1466","1467","1468","1469","1470","1471","1472","1473","1474","1475","1476","1477","1478","1479","1480","1481","1482","1483","1484","1485","1486","1487","1488","1489","1490","1491","1492","1493","1494","1495","1496","1497","1498","1499","1500","1501","1502","1503","1504","1505","1506","1507","1508","1509","1510","1511","1512","1513","1514","1515","1516","1517","1518","1519","1520","1521","1522","1523","1524","1525","1526","1527","1528","1529","1530","1531","1532","1533","1534","1535","1536","1537","1538","1539","1540","1541","1542","1543","1544","1545","1546","1547","1548","1549","1550","1551","1552","1553","1554","1555","1556","1557","1558","1559","1560","1561","1562","1563","1564","1565","1566","1567","1568","1569","1570","1571","1572","1573","1574","1575","1576","1577","1578","1579","1580","1581","1582","1583","1584","1585","1586","1587","1588","1589","1590","1591","1592","1593","1594","1595","1596","1597","1598","1599","1600","1601","1602","1603","1604","1605","1606","1607","1608","1609","1610","1611","1612","1613","1614","1615","1616","1617","1618","1619","1620","1621","1622","1623","1624","1625","1626","1627","1628","1629","1630","1631","1632","1633","1634","1635","1636","1637","1638","1639","1640","1641","1642","1643","1644","1645","1646","1647","1648","1649","1650","1651","1652","1653","1654","1655","1656","1657","1658","1659","1660","1661","1662","1663","1664","1665","1666","1667","1668","1669","1670","1671","1672","1673","1674","1675","1676","1677","1678","1679","1680","1681","1682","1683","1684","1685","1686","1687","1688","1689","1690","1691","1692","1693","1694","1695","1696","1697","1698","1699","1700","1701","1702","1703","1704","1705","1706","1707","1708","1709","1710","1711","1712","1713","1714","1715","1716","1717","1718","1719","1720","1721","1722","1723","1724","1725","1726","1727","1728","1729","1730","1731","1732","1733","1734","1735","1736","1737","1738","1739","1740","1741","1742","1743","1744","1745","1746","1747","1748","1749","1750","1751","1752","1753","1754","1755","1756","1757","1758","1759","1760","1761","1762","1763","1764","1765","1766","1767","1768","1769","1770","1771","1772","1773","1774","1775","1776","1777","1778","1779","1780","1781","1782","1783","1784","1785","1786","1787","1788","1789","1790","1791","1792","1793","1794","1795","1796","1797","1798","1799","1800","1801","1802","1803","1804","1805","1806","1807","1808","1809","1810","1811","1812","1813","1814","1815","1816","1817","1818","1819","1820","1821","1822","1823","1824","1825","1826","1827","1828","1829","1830","1831","1832","1833","1834","1835","1836","1837","1838","1839","1840","1841","1842","1843","1844","1845","1846","1847","1848","1849","1850","1851","1852","1853","1854","1855","1856","1857","1858","1859","1860","1861","1862","1863","1864","1865","1866","1867","1868","1869","1870","1871","1872","1873","1874","1875","1876","1877","1878","1879","1880","1881","1882","1883","1884","1885","1886","1887","1888","1889","1890","1891","1892","1893","1894","1895","1896","1897","1898","1899","1900","1901","1902","1903","1904","1905","1906","1907","1908","1909","1910","1911","1912","1913","1914","1915","1916","1917","1918","1919","1920","1921","1922","1923","1924","1925","1926","1927","1928","1929","1930","1931","1932","1933","1934","1935","1936","1937","1938","1939","1940","1941","1942","1943","1944","1945","1946","1947","1948","1949","1950","1951","1952","1953","1954","1955","1956","1957","1958","1959","1960","1961","1962","1963","1964","1965","1966","1967","1968","1969","1970","1971","1972","1973","1974","1975","1976","1977","1978","1979","1980","1981","1982","1983","1984","1985","1986","1987","1988","1989","1990","1991","1992","1993","1994","1995","1996","1997","1998","1999","2000","2001","2002","2003","2004","2005","2006","2007","2008","2009","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019","2020","2021","2022","2023","2024","2025","2026","2027","2028","2029","2030","2031","2032","2033","2034","2035","2036","2037","2038","2039","2040","2041","2042","2043","2044","2045","2046","2047","2048","2049","2050","2051","2052","2053","2054","2055","2056","2057","2058","2059","2060","2061","2062","2063","2064","2065","2066","2067","2068","2069","2070","2071","2072","2073","2074","2075","2076","2077","2078","2079","2080","2081","2082","2083","2084","2085","2086","2087","2088","2089","2090","2091","2092","2093","2094","2095","2096","2097","2098","2099","2100","2101","2102","2103","2104","2105","2106","2107","2108","2109","2110","2111","2112","2113","2114","2115","2116","2117","2118","2119","2120","2121","2122","2123","2124","2125","2126","2127","2128","2129","2130","2131","2132","2133","2134","2135","2136","2137","2138","2139","2140","2141","2142","2143","2144","2145","2146","2147","2148","2149","2150","2151","2152","2153","2154","2155","2156","2157","2158","2159","2160","2161","2162","2163","2164","2165","2166","2167","2168","2169","2170","2171","2172","2173","2174","2175","2176","2177","2178","2179","2180","2181","2182","2183","2184","2185","2186","2187","2188","2189","2190","2191","2192","2193","2194","2195","2196","2197","2198","2199","2200","2201","2202","2203","2204","2205","2206","2207","2208","2209","2210","2211","2212","2213","2214","2215","2216","2217","2218","2219","2220","2221","2222","2223","2224","2225","2226","2227","2228","2229","2230","2231","2232","2233","2234","2235","2236","2237","2238","2239","2240","2241","2242","2243","2244","2245","2246","2247","2248","2249","2250","2251","2252","2253","2254","2255","2256","2257","2258","2259","2260","2261","2262","2263","2264","2265","2266","2267","2268","2269","2270","2271","2272","2273","2274","2275","2276","2277","2278","2279","2280","2281","2282","2283","2284","2285","2286","2287","2288","2289","2290","2291","2292","2293","2294","2295","2296","2297","2298","2299","2300","2301","2302","2303","2304","2305","2306","2307","2308","2309","2310","2311","2312","2313","2314","2315","2316","2317","2318","2319","2320","2321","2322","2323","2324","2325","2326","2327","2328","2329","2330","2331","2332","2333","2334","2335","2336","2337","2338","2339","2340","2341","2342","2343","2344","2345","2346","2347","2348","2349","2350","2351","2352","2353","2354","2355","2356","2357","2358","2359","2360","2361","2362","2363","2364","2365","2366","2367","2368","2369"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,17,17,17,17,17,17,17,17,17,17,17,17,17,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,19,19,19,19,19,19,19,20,20,20,20,20,20,20,20,20,20,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,28,28,28,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,30,30,30,31,31,31,31,31,31,31,32,32,32,32,32,32,32,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,34,34,34,34,35,35,35,35,35,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,45,45,45,45,45,45,45,45,45,45,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,48,48,48,48,48,48,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,51,51,51,51,51,51,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,53,53,53,53,53,53,53,53,54,54,54,54,54,54,54,54,54,54,54,54,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,56,56,56,56,56,56,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,58,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,61,61,61,61,61,61,61,61,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,67,67,67,67,67,67,67,67,67,67,67,67,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68],[0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,1,1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,1,0,1,0,0,1,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,0,0,0,0,1,0,0,1,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0],[1.252762968,0.832909123,2.079441542,1.435084525,-0.916290732,0.955511445,1.722766598,1.568615918,0.262364264,1.280933845,1.667706821,3.186352633,1.280933845,1.280933845,0.262364264,3.688879454,0.916290732,1.410986974,3.165475048,0.641853886,-0.356674944,0.741937345,0.916290732,0.875468737,0.641853886,0.262364264,-2.302585093,2.58021683,1.667706821,1.252762968,1.704748092,-0.693147181,0.875468737,1.916922612,0.530628251,0.262364264,0.693147181,1.791759469,-0.223143551,1.435084525,2.32238772,4.34120464,1.098612289,-0.356674944,0.405465108,0.262364264,2.917770732,1.223775432,0.832909123,-0.510825624,1.098612289,-0.693147181,2.954910279,3.891820298,0.587786665,3.650658241,1.098612289,2.197224577,1.223775432,0.641853886,0.993251773,0.78845736,0.916290732,-0.223143551,-0.105360516,-0.916290732,0.641853886,0.470003629,4.028916757,0.832909123,3.328626689,1.252762968,0.09531018,0.09531018,1.335001067,1.386294361,0.405465108,-0.916290732,0.09531018,1.824549292,2.079441542,1.131402111,1.887069649,1.648658626,2.442347035,1.547562509,0.530628251,1.808288771,2.261763098,-0.223143551,2.041220329,1.808288771,2.079441542,-0.223143551,0.182321557,2.557227311,1.223775432,0.741937345,0.262364264,-0.510825624,0.336472237,0.955511445,2.028148247,1.098612289,0,2.2300144,1.774952351,1.131402111,1.722766598,-0.510825624,1.223775432,1.131402111,0.182321557,0.993251773,1.931521412,1.064710737,-0.105360516,-0.223143551,-0.105360516,-0.693147181,1.252762968,0.09531018,0.916290732,1.335001067,0.641853886,1.902107526,0.182321557,0.405465108,3.000719815,0.530628251,1.064710737,1.029619417,0,0.955511445,1.386294361,0,-0.223143551,0.405465108,0.693147181,-0.105360516,-0.510825624,0.993251773,0.993251773,0.182321557,0.470003629,0.741937345,2.451005098,-0.223143551,1.16315081,1.193922468,0.955511445,1.280933845,2.587764035,0.336472237,0.78845736,1.808288771,0.741937345,0.78845736,1.648658626,0.470003629,1.62924054,1.064710737,0.470003629,1.029619417,1.064710737,0.09531018,0.262364264,0.78845736,2.116255515,0,1.589235205,0.336472237,0.916290732,0,1.252762968,0.182321557,0,1.360976553,2.041220329,0.09531018,1.62924054,0.832909123,0.09531018,2.90690106,-1.203972804,-0.356674944,-2.302585093,0.741937345,0.693147181,1.458615023,0.641853886,0.09531018,0.262364264,0.262364264,-0.223143551,0.182321557,2.116255515,0.182321557,-0.510825624,2.595254707,-0.223143551,1.526056303,0.587786665,1.30833282,0.875468737,1.722766598,0,2.970414466,2.32238772,1.808288771,1.547562509,0.405465108,0.09531018,-0.105360516,0.875468737,-0.693147181,0.09531018,0.09531018,0.182321557,0.336472237,0.470003629,-0.356674944,-0.693147181,0.741937345,2.00148,0.336472237,2.208274414,2.104134154,0.875468737,0.470003629,1.526056303,0.993251773,1.064710737,1.029619417,-0.105360516,-0.356674944,1.098612289,2.014903021,0.955511445,0,0.875468737,1.064710737,2.251291799,4.520701029,1.131402111,0.09531018,-0.510825624,0.916290732,0.641853886,0.405465108,0.993251773,0.262364264,1.223775432,-0.356674944,0.530628251,-0.510825624,-0.356674944,0.262364264,0,0.641853886,1.704748092,0.955511445,1.547562509,1.704748092,0.262364264,-0.916290732,1.458615023,2.397895273,1.722766598,0.262364264,2.261763098,2.57261223,1.547562509,2.332143895,3.572345638,1.30833282,1.360976553,0.182321557,1.481604541,1.609437912,4.032469159,2.104134154,0.530628251,1.280933845,0.993251773,1.609437912,0.336472237,0,0.530628251,-0.223143551,-1.203972804,2.174751721,0.875468737,0.405465108,-0.223143551,1.029619417,1.16315081,0.832909123,-0.693147181,0.530628251,-0.510825624,1.193922468,2.197224577,0.09531018,0.336472237,2.451005098,0.916290732,2.667228207,1.85629799,1.667706821,1.808288771,3.206803244,0.336472237,0.78845736,3.449987546,0.955511445,2.261763098,1.064710737,1.526056303,0.693147181,1.386294361,2.57261223,0.993251773,1.30833282,1.704748092,1.945910149,-0.356674944,1.029619417,0.875468737,2.014903021,2.944438979,1.808288771,0.993251773,2.028148247,1.481604541,1.360976553,2.186051277,3.005682604,1.30833282,2.970414466,0.641853886,1.458615023,0.832909123,1.504077397,0.993251773,0.78845736,3.206803244,1.458615023,3.443618098,3.465735903,2.433613355,2.00148,2.104134154,0.875468737,2.041220329,1.029619417,0.993251773,1.824549292,0.78845736,0.262364264,0.78845736,0.832909123,0.916290732,3.370738174,0.262364264,0.641853886,1.435084525,1.029619417,0.916290732,2.667228207,0.641853886,2.844909384,1.589235205,0.916290732,1.916922612,2.014903021,0.78845736,0.916290732,3.077312261,0.530628251,0.336472237,-1.203972804,1.458615023,0.405465108,-0.105360516,3.126760536,2.104134154,1.840549633,2.091864062,3.380994674,2.128231706,1.30833282,3.526360525,0.741937345,0.955511445,4.001863709,1.16315081,1.30833282,2.292534757,3.414442608,1.808288771,1.30833282,1.16315081,0.182321557,0.262364264,0.693147181,1.568615918,0.693147181,-0.916290732,1.504077397,1.098612289,1.30833282,1.252762968,1.974081026,0.693147181,1.547562509,-1.609437912,0.693147181,1.609437912,1.648658626,1.280933845,0.832909123,2.174751721,1.131402111,0.405465108,1.974081026,0.336472237,-0.693147181,4.639571613,2.079441542,0.641853886,0.262364264,0.587786665,1.667706821,2.701361213,2.766319109,1.131402111,0.182321557,2.541601993,1.757857918,1.589235205,3.250374492,0.587786665,-0.105360516,1.757857918,2.985681938,0.182321557,2.839078464,1.589235205,1.808288771,0.587786665,0.262364264,0.832909123,0.182321557,2.282382386,0.916290732,3.538056564,2.2300144,-0.693147181,2.140066163,-0.510825624,2.970414466,1.029619417,0,0.530628251,1.568615918,0.955511445,1.960094784,3.0301337,2.740840024,2.379546134,0.262364264,2.292534757,3.471966453,4.222444565,2.610069793,2.00148,2.442347035,-2.302585093,2.251291799,3.165475048,0.916290732,1.435084525,1.547562509,1.722766598,2.624668592,0.470003629,0.530628251,0.405465108,0.916290732,1.481604541,0.09531018,0.832909123,0.916290732,0.262364264,0.182321557,0.405465108,-1.203972804,1.686398954,2.163323026,0.916290732,0.262364264,-0.356674944,-0.693147181,0,1.131402111,1.335001067,1.840549633,1.064710737,2.00148,3.261935314,2.397895273,1.916922612,0.875468737,0.262364264,-0.510825624,1.335001067,2.433613355,2.186051277,-0.510825624,2.054123734,0.182321557,-1.609437912,1.131402111,1.458615023,2.197224577,0,0.530628251,2.261763098,1.648658626,0.955511445,-1.203972804,1.62924054,3.407841924,1.808288771,1.410986974,2.809402695,0.470003629,3.912023005,1.360976553,2.610069793,2.809402695,-0.105360516,2.844909384,0.741937345,1.504077397,2.174751721,0.530628251,0.530628251,0.641853886,1.887069649,1.974081026,0,0.78845736,2.219203484,1.871802177,1.840549633,3.414442608,1.808288771,2.501435952,2.541601993,2.292534757,1.386294361,0.641853886,1.704748092,0.405465108,1.386294361,1.360976553,0.641853886,2.525728644,0.182321557,0.993251773,1.193922468,0.832909123,0.09531018,2.341805806,0,1.757857918,1.62924054,1.526056303,1.131402111,-0.510825624,2.091864062,0.993251773,-1.609437912,2.014903021,0.955511445,1.16315081,2.041220329,-0.693147181,-0.356674944,3.502549876,0.641853886,-0.223143551,1.064710737,0.741937345,0.916290732,0.78845736,1.704748092,1.30833282,0.470003629,0,1.098612289,0.262364264,0.262364264,0.336472237,0.693147181,3.058707073,1.30833282,0.470003629,3.46260601,1.504077397,3.424262655,3.280911216,1.740466175,0.875468737,3.08190997,1.648658626,2.653241965,3.342861805,3.353406718,-0.510825624,3.484312288,2.360854001,1.589235205,-0.693147181,1.410986974,1.757857918,1.386294361,1.029619417,1.945910149,0.587786665,0,-0.356674944,0.182321557,0.641853886,1.887069649,3.47506723,2.721295428,1.526056303,0.182321557,-2.302585093,2.251291799,0,0.470003629,1.62924054,2.219203484,1.568615918,-0.356674944,2.197224577,3.414442608,2.667228207,2.014903021,-0.510825624,1.386294361,4.001863709,-0.693147181,0.78845736,0.955511445,1.481604541,0.09531018,1.458615023,1.410986974,2.517696473,2.054123734,1.16315081,0.78845736,1.686398954,0.875468737,0.336472237,-0.223143551,2.2300144,0.530628251,1.335001067,1.987874348,3.421000009,3.218875825,1.974081026,-0.105360516,0.78845736,3.299533728,1.916922612,0.916290732,1.029619417,0.470003629,0.530628251,0.875468737,2.261763098,1.252762968,-0.693147181,0.693147181,0.470003629,1.064710737,0.693147181,2.660259537,2.895911938,1.029619417,0,0.916290732,4.310799125,1.410986974,1.193922468,1.987874348,-0.223143551,0.182321557,1.223775432,0.405465108,1.902107526,0.530628251,2.104134154,1.871802177,2.954910279,1.193922468,1.568615918,0.78845736,2.32238772,-0.510825624,0.641853886,0.741937345,0.405465108,0.262364264,0.405465108,0.641853886,2.839078464,0.78845736,0.693147181,-0.105360516,1.648658626,0.182321557,0.470003629,0.993251773,2.76000994,1.131402111,-0.356674944,0.916290732,-0.223143551,-0.510825624,0.693147181,-0.223143551,0,0.587786665,0.693147181,1.774952351,2.251291799,0.336472237,-0.223143551,1.547562509,2.803360381,1.931521412,1.648658626,2.890371758,1.410986974,1.223775432,2.960105096,3.749504076,1.193922468,0.09531018,1.098612289,0.262364264,-0.356674944,0.405465108,2.646174797,0.641853886,2.272125886,2.104134154,2.028148247,4.49088104,2.533696814,1.740466175,0.916290732,1.887069649,2.653241965,2.895911938,2.272125886,1.386294361,2.766319109,2.895911938,1.481604541,2.104134154,1.722766598,0.955511445,2.208274414,1.098612289,3.919991175,1.458615023,0.182321557,4.163559631,1.280933845,3.788724789,2.282382386,1.029619417,1.131402111,1.481604541,1.223775432,3.777348102,3.561046083,1.064710737,1.064710737,0.405465108,1.280933845,1.280933845,0.78845736,1.029619417,1.547562509,2.054123734,-1.203972804,-0.356674944,1.335001067,-2.302585093,0.405465108,-0.105360516,-0.105360516,2.251291799,-0.356674944,2.766319109,0.832909123,3.100092289,1.686398954,0.09531018,0.741937345,-0.356674944,-0.223143551,1.526056303,1.916922612,0.741937345,1.808288771,2.879198457,1.064710737,-0.510825624,1.410986974,1.064710737,0.405465108,0.262364264,-0.223143551,2.468099531,1.335001067,3.80666249,1.386294361,2.116255515,1.808288771,1.131402111,1.547562509,-0.223143551,0.587786665,0,0.336472237,2.360854001,1.029619417,-0.105360516,0.09531018,1.335001067,0.693147181,1.131402111,2.525728644,1.960094784,1.252762968,-0.356674944,0.470003629,1.064710737,2.014903021,-0.916290732,-1.203972804,-1.203972804,3.891820298,1.686398954,1.029619417,3.433987204,2.734367509,0.741937345,2.32238772,2.312535424,1.887069649,0.09531018,1.386294361,4.520701029,-0.510825624,0.09531018,1.774952351,-0.105360516,1.098612289,-0.223143551,0.336472237,0.78845736,0.78845736,1.280933845,-0.223143551,1.16315081,-0.105360516,0.262364264,2.424802726,0.470003629,-0.916290732,0.405465108,0.741937345,1.871802177,2.054123734,0.693147181,1.481604541,1.568615918,2.197224577,2.815408719,1.791759469,1.386294361,2.415913778,1.667706821,3.374168709,2.128231706,0.262364264,1.902107526,2.415913778,1.974081026,2.533696814,0.641853886,0.993251773,1.223775432,4.195697056,1.960094784,1.335001067,4.248495242,5.051777237,2.701361213,2.617395833,3.895893623,3.010620886,4.273884476,3.822098298,3.186352633,3.034952987,2.397895273,1.131402111,1.85629799,3.210843653,1.30833282,2.595254707,1.410986974,0.530628251,3.777348102,1.526056303,2.624668592,1.704748092,0.09531018,1.648658626,2.714694744,0.530628251,-0.916290732,2.57261223,2.949688335,0.832909123,3.718438256,5.611301622,0.09531018,2.240709689,1.410986974,0.741937345,1.740466175,2.844909384,2.90690106,1.029619417,0.262364264,1.223775432,2.459588842,0.09531018,2.525728644,0.262364264,3.411147713,3.310543013,2.674148649,1.193922468,-0.510825624,2.602689685,3.33220451,5.12455904,4.302712828,3.025291076,3.36729583,0.262364264,2.954910279,0.693147181,-0.105360516,2.58021683,0.09531018,4.587006215,1.791759469,1.360976553,2.844909384,4.183575696,2.251291799,1.504077397,1.098612289,3.27336401,0.336472237,0.955511445,-2.302585093,0.470003629,0.916290732,-2.302585093,-0.510825624,-0.916290732,0.955511445,-0.693147181,-2.302585093,0.916290732,1.280933845,-0.510825624,1.064710737,0.741937345,-0.105360516,0.262364264,2.468099531,2.219203484,-0.693147181,0,-1.203972804,-0.105360516,-0.105360516,0.955511445,0.875468737,-0.916290732,1.481604541,-0.356674944,0.587786665,0.09531018,0,1.193922468,0,0.336472237,5.564137192,0.336472237,0.470003629,-1.609437912,2.312535424,1.252762968,0.741937345,3.198673118,-0.223143551,1.131402111,1.029619417,-0.223143551,-0.356674944,-0.916290732,1.029619417,1.360976553,0,-0.693147181,3.826465117,1.458615023,-0.223143551,-1.609437912,2.772588722,2.174751721,0.916290732,0.336472237,0.405465108,0.693147181,0.470003629,1.667706821,1.960094784,2.104134154,3.202746443,-2.302585093,2.541601993,0.262364264,-1.609437912,2.468099531,0,-2.302585093,0,0.530628251,-2.302585093,1.648658626,3.18221184,3.45946629,0.875468737,-0.223143551,1.808288771,2.054123734,1.667706821,-0.693147181,-0.223143551,0.182321557,1.360976553,-0.105360516,0.336472237,1.280933845,1.774952351,-1.609437912,-0.105360516,0.530628251,0.262364264,1.960094784,0.262364264,2.128231706,0.741937345,0.262364264,-0.916290732,2.57261223,-0.223143551,-0.510825624,-0.356674944,2.76000994,0.262364264,0.641853886,0.641853886,-0.916290732,-0.510825624,-0.916290732,-0.105360516,-0.105360516,-0.223143551,0.405465108,1.386294361,-0.510825624,-0.510825624,-0.916290732,2.541601993,1.386294361,-0.356674944,0.182321557,0.530628251,0.641853886,3.010620886,1.589235205,-0.105360516,-0.693147181,0.78845736,1.193922468,3.091042453,-0.105360516,1.064710737,-0.356674944,2.272125886,0.641853886,2.00148,1.252762968,0.262364264,0.530628251,0.470003629,2.90690106,0.470003629,0.530628251,-0.223143551,0.405465108,1.098612289,3.18221184,1.252762968,3.817712326,2.370243741,0.530628251,3.33220451,2.197224577,2.557227311,2.862200881,2.965273066,0.336472237,0.875468737,1.335001067,2.388762789,2.884800713,2.501435952,-0.223143551,1.648658626,2.985681938,0.832909123,1.435084525,2.76000994,4.168214411,0.470003629,0.832909123,-2.302585093,1.193922468,1.648658626,1.757857918,-0.223143551,-0.105360516,1.740466175,0.641853886,1.85629799,0.916290732,1.098612289,2.282382386,1.840549633,2.753660712,0.09531018,0.09531018,0.693147181,0.182321557,1.458615023,0.641853886,0.182321557,1.223775432,0,2.00148,3.258096538,0.262364264,0.182321557,2.610069793,0.262364264,1.064710737,2.341805806,2.079441542,1.360976553,1.686398954,2.990719732,-0.223143551,0.916290732,-0.510825624,1.871802177,2.251291799,2.332143895,0.832909123,2.379546134,0.916290732,0.470003629,0.530628251,-0.223143551,2.272125886,0.741937345,1.131402111,0,0.09531018,0.470003629,2.054123734,-0.105360516,0.405465108,1.945910149,1.386294361,0.182321557,-0.105360516,0.262364264,-0.223143551,0.405465108,2.140066163,0,-2.302585093,-0.356674944,0.741937345,0.182321557,1.458615023,-0.105360516,1.648658626,1.193922468,-1.203972804,1.547562509,1.902107526,0.336472237,1.335001067,-0.510825624,0.78845736,0.262364264,-0.693147181,0.955511445,1.824549292,1.098612289,0.955511445,1.547562509,0.182321557,1.131402111,0,0.182321557,0.09531018,0.182321557,1.360976553,0,1.722766598,2.140066163,-0.356674944,-0.510825624,1.335001067,0.641853886,-0.510825624,1.360976553,0.693147181,0.336472237,-0.105360516,-0.356674944,1.410986974,1.064710737,0.641853886,0.336472237,0.262364264,2.4765384,-0.223143551,1.435084525,0.641853886,0.832909123,1.064710737,-0.356674944,0.587786665,0.336472237,0.09531018,1.131402111,2.00148,1.360976553,-0.223143551,0,0.916290732,2.517696473,1.609437912,2.57261223,2.066862759,1.85629799,1.808288771,3.310543013,1.16315081,1.568615918,2.251291799,2.985681938,2.912350665,0.405465108,1.945910149,3.131136911,2.079441542,2.797281335,3.317815773,4.660604893,2.772588722,2.797281335,2.251291799,3.095577609,1.568615918,1.757857918,3.411147713,3.591817741,2.351375257,2.219203484,1.458615023,2.76000994,2.587764035,0.993251773,2.116255515,1.931521412,2.564949357,2.990719732,2.174751721,3.555348061,1.648658626,2.240709689,2.282382386,0.641853886,1.871802177,2.541601993,1.774952351,3.889777396,2.151762203,2.406945108,2.442347035,4.198704578,2.694627181,2.406945108,3.532225644,1.808288771,1.547562509,1.740466175,1.568615918,0.405465108,2.028148247,1.704748092,1.098612289,2.2300144,2.76000994,2.2300144,0.693147181,2.687847494,1.098612289,1.193922468,0.09531018,0.693147181,-0.356674944,0.405465108,-2.302585093,1.098612289,1.648658626,0.182321557,1.16315081,1.526056303,0.262364264,0.530628251,1.223775432,0.262364264,0.182321557,1.609437912,0,0.262364264,0.741937345,2.197224577,0.405465108,1.667706821,0.182321557,1.16315081,0,0.693147181,-0.105360516,1.223775432,1.504077397,0.336472237,0.916290732,1.589235205,1.335001067,1.871802177,0.832909123,0.336472237,0.832909123,2.890371758,0.405465108,0.955511445,0,0.693147181,2.292534757,0.405465108,2.104134154,1.589235205,1.686398954,0.916290732,2.370243741,2.595254707,-0.916290732,0.993251773,1.335001067,2.282382386,2.595254707,3.234749174,1.029619417,5.281679725,3.020424886,2.48490665,2.442347035,3.552486829,2.302585093,2.32238772,1.791759469,2.442347035,2.140066163,2.509599262,3.873282177,1.360976553,2.778819272,2.602689685,2.151762203,2.186051277,1.131402111,1.648658626,2.397895273,3.339321978,2.912350665,1.740466175,1.609437912,1.131402111,3.589059119,3.843744165,1.757857918,2.2300144,1.757857918,1.547562509,2.564949357,0.182321557,3.660994251,2.261763098,-0.693147181,2.140066163,4.356708827,1.974081026,1.62924054,3.161246712,1.871802177,-1.203972804,2.557227311,-0.510825624,0.832909123,0.182321557,0.262364264,0.182321557,0.09531018,1.435084525,0.875468737,1.824549292,-0.105360516,-0.105360516,1.840549633,0.530628251,1.098612289,0.693147181,0.993251773,2.116255515,3.100092289,0.832909123,0.993251773,0.693147181,1.335001067,0.916290732,2.533696814,0.470003629,1.435084525,1.589235205,2.397895273,-0.693147181,0.875468737,1.064710737,0.693147181,1.252762968,0.693147181,1.131402111,0.262364264,0.530628251,0.336472237,0.182321557,0.09531018,1.252762968,0.262364264,0.875468737,2.867898902,0.693147181,-0.510825624,2.151762203,1.960094784,0.916290732,1.589235205,0.875468737,1.609437912,-0.105360516,2.564949357,1.386294361,0.78845736,0.641853886,0.530628251,-0.105360516,0.470003629,0.955511445,0.405465108,0.405465108,0.832909123,0.993251773,0.955511445,1.029619417,2.721295428,0.262364264,0.641853886,2.054123734,1.252762968,0.262364264,0.875468737,-0.510825624,-0.356674944,0.587786665,0.405465108,0.955511445,0.09531018,1.740466175,0.530628251,1.871802177,0.09531018,2.261763098,0.955511445,0.530628251,1.686398954,0.955511445,0.587786665,1.458615023,0.262364264,1.547562509,0.741937345,-0.105360516,0,0.587786665,0.587786665,-0.105360516,-0.105360516,-0.693147181,0.530628251,0.955511445,1.960094784,0.875468737,0.182321557,2.251291799,2.116255515,0.530628251,0.78845736,1.098612289,0.741937345,-0.693147181,2.028148247,1.410986974,-0.916290732,1.960094784,-0.223143551,1.098612289,0.955511445,0.336472237,1.740466175,1.458615023,0.09531018,0.832909123,2.415913778,1.360976553,4.188138442,0.470003629,2.240709689,1.435084525,1.16315081,0.587786665,0.336472237,2.753660712,1.223775432,4.348986781,0.587786665,0.182321557,3.985273467,2.028148247,0.916290732,1.791759469,1.526056303,0.641853886,0.530628251,0.875468737,-2.302585093,-0.916290732,-1.203972804,-0.693147181,-0.105360516,0.262364264,1.704748092,2.944438979,1.410986974,0.993251773,0.832909123,-1.203972804,-0.105360516,0.78845736,1.481604541,0.262364264,-0.105360516,-0.105360516,0.955511445,1.360976553,1.85629799,1.252762968,1.16315081,4.536891345,2.533696814,0.741937345,0.641853886,0.182321557,0.262364264,0.641853886,0.530628251,-0.510825624,-0.510825624,0,1.335001067,0.955511445,1.16315081,1.30833282,0.262364264,1.686398954,0.741937345,0.587786665,0.336472237,0.182321557,-0.223143551,0.955511445,0.530628251,0.336472237,0.78845736,-0.105360516,1.064710737,-0.105360516,-0.223143551,2.493205453,2.272125886,1.131402111,0.530628251,1.30833282,2.091864062,0.470003629,-0.356674944,1.686398954,1.193922468,0.641853886,-0.223143551,0.530628251,0.641853886,3.618993327,2.517696473,0.336472237,2.63188884,1.458615023,2.140066163,1.223775432,3.377587516,3.104586678,1.131402111,1.568615918,-0.510825624,2.360854001,0.470003629,0.955511445,2.00148,-0.693147181,2.827313622,4.151039906,-0.356674944,-0.356674944,1.504077397,1.609437912,1.064710737,-0.916290732,0.470003629,1.722766598,2.091864062,0.182321557,1.098612289,3.054001182,0.405465108,1.064710737,1.808288771,2.557227311,-0.693147181,2.58021683,2.041220329,2.312535424,1.16315081,2.054123734,2.282382386,1.131402111,0.916290732,2.58021683,2.687847494,1.481604541,3.46885603,-0.223143551,1.029619417,0.875468737,0.875468737,1.252762968,-0.105360516,0.955511445,2.186051277,0.693147181,0,1.547562509,0.470003629,1.945910149,2.219203484,1.62924054,1.30833282,1.609437912,0.916290732,-0.223143551,0.875468737,0.875468737,2.174751721,0.955511445,1.360976553,-0.356674944,1.335001067,2.116255515,1.193922468,1.386294361,0.262364264,1.16315081,0.78845736,1.757857918,0.78845736,0.405465108,1.62924054,-0.223143551,-0.105360516,0.875468737,0.09531018,0.09531018,-2.302585093,1.609437912,0.336472237,1.335001067,1.098612289,0.832909123,-0.223143551,0.09531018,0.09531018,0.262364264,0.955511445,1.871802177,1.131402111,0.09531018,1.481604541,1.945910149,1.481604541,-0.916290732,-0.356674944,-0.510825624,2.57261223,1.589235205,1.504077397,0.832909123,1.704748092,1.064710737,4.298645026,0.262364264,3.711130063,2.63188884,1.568615918,2.028148247,2.028148247,1.791759469,1.704748092,3.044522438,3.740047741,2.312535424,2.208274414,1.791759469,2.63188884,1.481604541,1.902107526,2.360854001,2.00148,1.840549633,2.809402695,1.526056303,0.875468737,3.94931879,1.740466175,-0.693147181,2.856470206,0.641853886,0.470003629,0.336472237,4.378269586,1.931521412,1.757857918,1.029619417,2.091864062,2.778819272,2.867898902,1.568615918,1.131402111,2.388762789,0.741937345,0.530628251,3.725693427,2.797281335,0.832909123,-2.302585093,-0.356674944,1.280933845,-0.105360516,0.993251773,0.875468737,1.064710737,0.641853886,1.029619417,-0.356674944,-1.203972804,-0.356674944,0.993251773,0.693147181,0.78845736,2.76000994,-0.693147181,-0.356674944,-0.510825624,0.405465108,1.029619417,0.832909123,0.741937345,0.09531018,-0.916290732,1.526056303,1.30833282,1.16315081,0.741937345,0.875468737,0.641853886,0.405465108,-0.693147181,0.955511445,-1.203972804,0.587786665,-0.223143551,0.530628251,-0.510825624,0.530628251,0.693147181,-0.356674944,0.470003629,-1.203972804,1.704748092,-0.510825624,2.564949357,-0.916290732,1.335001067,1.458615023,3.629660094,1.686398954,0.09531018,0.405465108,1.131402111,1.757857918,0.470003629,-0.693147181,-0.510825624,1.029619417,0.832909123,1.223775432,0.09531018,-0.105360516,1.504077397,-1.609437912,0.262364264,-0.693147181,-0.916290732,0.587786665,0.09531018,0,0.587786665,0.182321557,-0.693147181,-0.223143551,0.530628251,-1.609437912,0.182321557,0.405465108,-0.693147181,1.360976553,-0.916290732,-1.203972804,-0.693147181,0.693147181,0.262364264,-1.609437912,-0.356674944,-0.356674944,1.85629799,0.262364264,0.78845736,0.993251773,1.16315081,0.470003629,1.85629799,0.641853886,1.704748092,0.09531018,-0.105360516,0.693147181,1.16315081,0,0.09531018,0.09531018,0.741937345,0.182321557,0.587786665,0.262364264,1.722766598,-0.693147181,0.78845736,0.182321557,-0.223143551,-1.203972804,-0.105360516,-1.609437912,0.916290732,0.741937345,0.405465108,-0.693147181,1.481604541,-0.105360516,0.182321557,-0.356674944,-0.693147181,1.098612289,2.797281335,1.960094784,0.832909123,1.931521412,-0.693147181,-0.693147181,-0.223143551,2.208274414,1.410986974,0.587786665,5.425830687,0.587786665,1.704748092,1.280933845,1.193922468,5.109575242,1.098612289,1.504077397,1.098612289,-0.916290732,0.182321557,-0.510825624,3.990834186,2.2300144,1.62924054,0.875468737,2.066862759,3.299533728,0.336472237,3.756538103,0.09531018,3.645449896,-0.510825624,-1.203972804,-0.916290732,2.00148,0.832909123,2.928523524,2.054123734,3.779633817,0.336472237,0.78845736,1.360976553,1.526056303,0.262364264,0.587786665,2.332143895,3.73289634,2.041220329,2.174751721,1.504077397,4.295923936,-0.356674944,2.653241965,2.041220329,2.116255515,0.693147181,3.899950424,0.587786665,1.386294361,1.029619417,-0.916290732,-0.223143551,0.09531018,0.641853886,1.252762968,1.16315081,1.16315081,1.410986974,0.78845736,1.098612289,0.693147181,1.945910149,3.295836866,2.388762789,-2.302585093,0.182321557,0.09531018,0.336472237,1.064710737,1.808288771,0.693147181,1.064710737,0.09531018,-0.510825624,0.336472237,0.955511445,2.351375257,-0.105360516,0.993251773,0.955511445,1.871802177,0.78845736,2.433613355,1.667706821,1.667706821,2.694627181,2.856470206,0.693147181,-0.223143551,-0.223143551,-0.223143551,0.78845736,1.589235205,-0.223143551,1.435084525,4.249922794,-0.356674944,1.223775432,-1.203972804,-0.356674944,0,0.470003629,2.00148,1.757857918,0.875468737,1.193922468,0.470003629,-0.356674944,3.317815773,3.148453361,-0.510825624,0.470003629,1.435084525,-1.203972804,3.126760536,0.470003629,-0.510825624,3.881563798,0.182321557,0.693147181,1.902107526,2.525728644,1.902107526,0,2.208274414,2.442347035,5.482304203,0.693147181,0.470003629,0.916290732,3.860729711,2.912350665,0.336472237,-0.356674944,0.530628251,2.00148,2.653241965,3.113515309,1.386294361,1.131402111,1.098612289,2.014903021,-1.203972804,0.832909123,2.32238772,1.064710737,1.16315081,0.405465108,-0.105360516,1.458615023,0.405465108,0.336472237,0.09531018,1.223775432,-0.510825624,1.931521412,-0.693147181,-1.203972804,0.587786665,-0.105360516,-0.223143551,3.39785848,2.219203484,-0.223143551,2.041220329,2.116255515,0.832909123,0.336472237,0.587786665,0.78845736,0.470003629,0.587786665,0.336472237,0.641853886,-0.693147181,0.875468737,0.587786665,0.405465108,0.405465108,0.470003629,0.587786665,-0.223143551,3.020424886,1.435084525,-0.693147181,2.610069793,0.530628251,0.587786665,1.280933845,0.182321557,2.493205453,0.470003629,0.470003629,1.887069649,0.262364264,0.182321557,1.887069649,1.360976553,2.282382386,1.252762968,-2.302585093,0.78845736,2.2300144,0.741937345,0.741937345,1.704748092,1.098612289,0.262364264,-2.302585093,0.955511445,0.182321557,2.610069793,1.064710737,0.641853886,1.547562509,1.648658626,0.587786665,2.151762203,1.504077397,0.262364264,0.336472237,0.641853886,0.993251773,-0.510825624,-0.510825624,2.57261223,1.589235205,-1.609437912,0.09531018,-0.105360516,0.182321557,0,-1.609437912,-0.105360516,-0.916290732,1.131402111,0.262364264,0.78845736,0.182321557,3.126760536,-0.356674944,-1.203972804,-2.302585093,0.405465108,0.641853886,2.282382386,0.741937345,0.336472237,-0.223143551,0.530628251,0.641853886,-0.105360516,0.09531018,1.791759469,1.916922612,1.029619417,-0.105360516,2.140066163,1.458615023,0.09531018,1.335001067,1.547562509,2.653241965,1.458615023,1.193922468,1.85629799,1.064710737,1.16315081,2.32238772,0,1.62924054,2.00148,-0.356674944,0.741937345,2.219203484,-0.916290732,0.741937345,0.530628251,1.064710737,-0.105360516,2.028148247,-0.105360516,3.535145354,-0.105360516,1.704748092,0.741937345,2.451005098,-2.302585093,2.292534757,1.30833282,1.410986974,2.360854001,1.252762968,0.530628251,2.140066163,0.832909123,0.530628251,0.262364264,1.280933845,1.193922468,-0.693147181,0.09531018,1.589235205,0,0.470003629,1.481604541,1.887069649,0.336472237,0.336472237,0.336472237,1.824549292,1.280933845,0.530628251,0.78845736,0.641853886,1.16315081,0.336472237,0.405465108,-0.356674944,0.916290732,0.587786665,0.693147181,0.470003629,3.854393893,2.163323026,2.433613355,1.458615023,-0.223143551,2.151762203,2.509599262,0.405465108,3.61361697,1.740466175,1.568615918,2.186051277,2.93385687,1.193922468,0.405465108,4.197201948,1.098612289,1.589235205,2.93385687,5.047288612,0.587786665,1.064710737,4.529368473,-0.105360516,0.336472237,-0.105360516,1.064710737,2.240709689,0.78845736,3.113515309,2.549445171,3.832979798,2.104134154,2.541601993,3.19047635,1.667706821,3.517497837,1.887069649,1.029619417,4.027135813,1.458615023,1.16315081,1.887069649,1.902107526,1.335001067,2.617395833,1.386294361,0.875468737,2.251291799,4.228292535,1.589235205,1.064710737,2.602689685,3.169685581,2.995732274,0.182321557,1.648658626,1.360976553,3.42751469,2.116255515,1.871802177,1.945910149,2.646174797,1.648658626,0.336472237,3.068052935,3.054001182],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>county<\/th>\n      <th>floor<\/th>\n      <th>radon<\/th>\n      <th>cgroup<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>

???
see the example 2 , the response variable is radon which means one kind of  harmful substance's concentration. the independent variable is county and floor. as you could see there are  68 county, and the floor of 0 row  means the concentration of radon on the ground , 1 means the concentration of radon on the first floor. don't care about the cgroup, it would not be used in this example. our question is about : is the concentration of radon  different  from the ground and the first floor????
---
class: center, middle
# modeol 1
## linear model
???
first we try to buid the simplest model

---
.panelset[
.panel[.panel-name[Code-lm]



```r
radon &lt;- read.csv('E:/academic_resources/note-tutorial/data/Radon_Data_RB.csv', h=TRUE)
radon$floor &lt;- as.factor(radon$floor)
radon$county &lt;- as.factor(radon$county)
radon$cgroup &lt;- as.factor(radon$cgroup)

mod.radon.lm1 &lt;- lm(radon~floor, data=radon)

summary(mod.radon.lm1) 
```
]

.panel[.panel-name[Result-lm]

```
## 
## Call:
## lm(formula = radon ~ floor, data = radon)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5419 -0.8338 -0.1407  0.7888  4.3720 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.23931    0.02629  47.145   &lt;2e-16 ***
## floor1      -0.74801    0.08125  -9.207   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.211 on 2367 degrees of freedom
## Multiple R-squared:  0.03457,	Adjusted R-squared:  0.03417 
## F-statistic: 84.76 on 1 and 2367 DF,  p-value: &lt; 2.2e-16
```
]
]
???
- C-l: put all the data to the matrix called radon,set the Categorical Variable as factor, Write the fomular for the linear model, y is radon, x is floor, and the data we use from the matrix of radon, use the function summary to answer our question.
- R-l: the p value told us :yes!! about the concentration of radon, there a huge diffence between the ground and first floor!! so do you think these results are correct???????? NO!!!note the value of R-squared, only 0.03!!!! this means the linear relationship between y and x explains only 0.03 of the total variation in y！！in another words， the model we build almost can not  explain the data we have !! such kind of problem of would appear just because our model too simple and at the same time, the number of our samples are too much, see back sheet！！！we have more than two thousands samples, when model is simple ,but the number of sample is large, we get very samll standard error, and then we get  very small p value, so no matter what is the real relationship between x and y, when we have a lot of samples ,the p value is always significant, but the model is Inappropriate!!!!so how could we deal with the problem??

---
class: center, middle
# &lt;font color=red&gt;add the  independent variable !!! &lt;/font&gt;
???
just add more independent variable!
---
class: center, middle
# modeol 2
## linera mixed model
???
and let's build another model
---
.panelset[
.panel[.panel-name[Code-plot]

```r
library(ggplot2)
#right image
ggplot(data=radon, aes(x=floor, y=radon)) + geom_point() + geom_smooth(method='lm')
#left image
ggplot(data=radon, aes(x=floor, y=radon, group=county)) +
  geom_point() +
  geom_smooth(method='lm') +
  facet_wrap(~county) 
#Random effect and fixed effect????
```
]

.panel[.panel-name[plot]

.pull-left[
![](AdvanceSta-slides_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]


.pull-right[
![](AdvanceSta-slides_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]
]

.panel[.panel-name[Code-lmer1]

```r
library(lme4)
mod.radon.lmer1 &lt;- lmer(radon~floor + (1|county), data=radon)
summary(mod.radon.lmer1) 
```
]

.panel[.panel-name[Result-lmer1]

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: radon ~ floor + (1 | county)
##    Data: radon
## 
## REML criterion at convergence: 7282.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.5868 -0.6617 -0.0608  0.5875  4.0997 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  county   (Intercept) 0.2567   0.5067  
##  Residual             1.1997   1.0953  
## Number of obs: 2369, groups:  county, 68
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  1.33689    0.06981   19.15
## floor1      -0.79302    0.07507  -10.56
## 
## Correlation of Fixed Effects:
##        (Intr)
## floor1 -0.125
```
]

.panel[.panel-name[Code-lmer2]

```r
library(lme4)
mod.radon.lmer2 &lt;- lmer(radon ~ floor + (1+floor|county), data=radon)
summary(mod.radon.lmer2) 
```
]

.panel[.panel-name[Result-lmer2]

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: radon ~ floor + (1 + floor | county)
##    Data: radon
## 
## REML criterion at convergence: 7281.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.5980 -0.6563 -0.0527  0.5868  4.1084 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  county   (Intercept) 0.26355  0.5134        
##           floor1      0.03661  0.1913   -0.35
##  Residual             1.19632  1.0938        
## Number of obs: 2369, groups:  county, 68
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  1.34126    0.07063   18.99
## floor1      -0.81147    0.08077  -10.05
## 
## Correlation of Fixed Effects:
##        (Intr)
## floor1 -0.217
```
]
.panel[.panel-name[Compare-value]
.pull-left[

```r
rbind(fixef(mod.radon.lmer1),fixef(mod.radon.lmer2))
```

```
##      (Intercept)     floor1
## [1,]    1.336888 -0.7930172
## [2,]    1.341265 -0.8114683
```
]
.pull-right[

```r
VarCorr(mod.radon.lmer1) 
```

```
##  Groups   Name        Std.Dev.
##  county   (Intercept) 0.50669 
##  Residual             1.09531
```

```r
VarCorr(mod.radon.lmer2)
```

```
##  Groups   Name        Std.Dev. Corr  
##  county   (Intercept) 0.51338        
##           floor1      0.19133  -0.345
##  Residual             1.09376
```
]
]

.panel[.panel-name[Compare-plot]
![](AdvanceSta-slides_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

]
]
???
- C-p！！！before we buid the linear mixed model, we plot first . we draw the right image, the black points is the raw data of radon and floor, and we would use the lm method to draw the line in oder to  show  the relationship of radon bwtween the ground and floor 1, PLOT!!! 
- PLOT!!!as we could see, in the linear model , the concentration of radon on the groud is higher!!!! but we should know these more than two thousands date from more than 60 county!!and we have already known the lm model is not appropriate. so when we plot the image according to the different county, we would find in some county ,the concentration of radon on first floor is higher! but in some county ,there is no difference, and even in some county a lot of data is missing. so we could see clearly the variable of county has a great influence on the the relationship about radon bwtween the ground and floor 1!!! 
- C-p!!!! so from this example we could see , when the variable is Directly related to our question or hypothesis, and there are enough information in it and it is experimental manipulations, we choose it as fixed factor, just like the variable "floor" in these example. and if the variable  is not our main concern and maybe we don't have enough information about it but it still have some inluence on the variation of response variable, this would help us to increase  our test power,then we set it as radom factor, just like the variable of county in our example!!! when we think about these two types of effect, the model we build would be called linear mixd model!!
- C-l1!!!there are also two types of radom effct in the mixed model, let's see the first one,in R we set the radom effect like this,and the floor is the fixed factor. In front of the vertical line, there is only the number of 1 which means there are the same slope of leverls inside the factor of ramdom , this  is called random intercept models .
- C-l2!!!!, but in this model In front of the vertical line, there are number of 1 and the fixed factor  which means there are differnt  slope of leverls inside the factor of ramdom.  and this is called  random slope models!!i would draw a picture to show their differnce,-------so in conclusion, in the radom intercept models, among the differnt county , the concentration of radon on the ground is diffent, but the differances between  the ground and floor 1 are the same , but in this random slope models, among the differnt county , the concentration of radon on the ground is diffent as well as differances between  the ground and floor 1 are differnt!!!!
- Result!!!!!!: when we summary these two types of mixed moedls ,we could the result there are 2 parts ,one part  is about radom effect  and it told us about the  components of variance, why the infomation is so limiited because when we estimate the radom factor's parameter, we assume it is consistent with a normal distribution and its mean value is 0!!! another part is about fixed effect , it only shows the estimate and t value, there is no p value?? why?? i would explain later.
- Compare,let's compare these two models' resutls, (left!!)as you can see the intercpet reprecent floor0, and because of the diffent types of radom models, so their estimates are a little bit differnt, this is the part of  fixed effect , as for the part of  radom  effect, (right!!!)see here, we could see  when we use the radom slope models, the std deviance of residual is smaller!!!that means in the unexplained part of the total variation of response variable becomes less!!but the question is no matter in this part or in the fixed effect part we don' t have enough imformation to decide should I remove some independent variable? or the model i build is good enough??
---
class: center, middle
# &lt;font color=red&gt;How to evaluate lmm ??? &lt;/font&gt;

???
in order to answer these questions, i would give the third example, because the in the former one, all of the independent variables are factors, that is not easy to understand.
---
## example 3
<div id="htmlwidget-27a6251d00cbd347f90b" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-27a6251d00cbd347f90b">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103"],[1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10],[5,7,9,10,12,13,14,15,18,19,22,23,5,7,9,11,12,13,15,16,19,22,24,1,3,6,10,12,14,15,16,18,19,23,1,3,4,5,7,9,10,12,13,14,19,21,3,5,6,7,9,11,12,14,17,18,19,20,1,6,7,9,11,12,16,17,2,3,6,9,10,14,15,23,3,7,10,11,13,14,19,21,22,5,9,10,15,16,20,22,24,2,3,8,9,12,13,15,18,19,21,23,24],["hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn","hn"],["L","L","L","L","L","L","L","L","L","L","L","L","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","L","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","D","L","L","L","L","L","L","L","L","L","D","D","D","D","D","D","D","D","L","L","L","L","L","L","L","L","L","L","L","L"],[0,0,0,0.25,0.25,0.25,0.1,0.1,0.25,0.1,0.1,0,0.25,0.1,0.1,0.1,0,0,0,0,0.25,0.1,0.25,0.1,0,0.25,0.25,0.1,0.25,0,0,0,0.1,0.1,0.1,0,0.1,0.25,0.25,0,0.1,0.25,0,0,0.25,0.1,0,0.1,0.1,0,0.25,0,0.25,0.25,0.1,0,0.1,0.25,0,0.1,0.1,0,0.25,0.1,0.25,0.25,0,0.25,0.1,0,0,0.25,0.1,0.25,0.1,0.1,0.25,0.25,0,0.25,0,0,0,0.1,0,0,0.1,0.1,0.25,0.25,0.1,0.1,0,0.1,0.1,0.25,0,0,0.25,0.25,0.1,0,0.25],[-0.0752481,3.3149585,8.549523,5.1861251,2.09193,1.4813057,-1.1771277,7.5755527,1.9539824,3.7577667,5.7964055,13.1485255,-1.8169207,2.9504451,-0.8733739,-0.1823842,2.1298488,1.4664364,2.8113515,0.7036389,-2.2096605,-0.0083978,0.9389882,-0.3427075,1.0133993,-1.1332131,-0.8329182,0.8030257,0.6116822,2.8172785,5.076102,1.5626426,-0.540557,0.8220591,4.6574995,14.2741424,7.9709523,0.7491061,-0.0369281,5.9117597,7.4958859,0.5532096,15.012556,7.8182224,1.5943729,10.5090888,14.3483185,8.5055931,9.295789,18.566262,4.8530393,6.5622034,3.3982022,4.8887791,15.5984096,21.926829,5.8863498,0.519636,1.398989,1.6359123,-1.4759372,3.5779123,1.2281263,-1.2824708,1.0757226,-1.2663269,0.6447824,-0.6887108,3.1017049,-1.5238807,1.3153729,0.8369692,-0.9328774,-1.3938869,0.0201358,0.0322183,8.4156686,2.1937061,4.6715457,2.5771156,6.2942698,10.8723428,13.5479259,3.7375259,1.2599094,2.7719127,-0.9164906,2.4719993,-1.3122374,-2.3342197,1.0614228,7.6649424,9.4952698,9.5842657,12.7105057,6.923749,5.9934859,14.0784455,2.2085065,1.3261626,4.8164121,8.9716121,3.9575399],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>shadehouse<\/th>\n      <th>sld.no<\/th>\n      <th>species<\/th>\n      <th>light<\/th>\n      <th>damage<\/th>\n      <th>growth<\/th>\n      <th>survival<\/th>\n      <th>block<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2,5,6,7,8]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
???
here is the example 3 ,
the response variable is the growth of seedings , and the fixed effect are light and damage, the radom effect is shadehouse. now we have 10   shadehouse , in 5 shedehouse we create the enviroment with light, and this L, in ohter 5 shadehouse we create the enviroment without light, and this is D means dark. in each shadehouse, there a lot of seedings, We give them different degrees of damage to the leaves.  so we want to konw how the light and damge to effect the growth of seedlings, and we also assume that there are a little bit difference which we can't control between these 10 shadehouses  and that would  impact our results. so we set the shadehouse as radom effect. 
---
class: center, middle
# &lt;font color=red&gt;Q1: Is the fixed effect important ??? &lt;/font&gt;
???
we have to test the fixed effect and radom effect Separately so the first question is about the fixed effect
---
.panelset[
.panel[.panel-name[Code-model]

```r
damage &lt;- read.csv("E:/academic_resources/note-tutorial/data/plantdamage3.csv")
damage$shadehouse &lt;- as.factor(damage$shadehouse)
damage$light &lt;- as.factor(damage$light)
library(lme4)
mod1&lt;-lmer(growth~light*damage+(1+damage|shadehouse),damage)
summary(mod1)
anova(mod1)
library(lmerTest)
anova(mod1)
```
]

.panel[.panel-name[Result-summary]

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: growth ~ light * damage + (1 + damage | shadehouse)
##    Data: damage
## 
## REML criterion at convergence: 512
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.50490 -0.48897 -0.07229  0.57284  2.71867 
## 
## Random effects:
##  Groups     Name        Variance Std.Dev. Corr 
##  shadehouse (Intercept)  5.292   2.300         
##             damage      79.497   8.916    -1.00
##  Residual                8.652   2.941         
## Number of obs: 103, groups:  shadehouse, 10
## 
## Fixed effects:
##               Estimate Std. Error t value
## (Intercept)      1.677      1.227   1.367
## lightL           8.238      1.704   4.835
## damage          -9.460      5.933  -1.595
## lightL:damage  -19.368      8.073  -2.399
## 
## Correlation of Fixed Effects:
##             (Intr) lightL damage
## lightL      -0.720              
## damage      -0.868  0.625       
## lightL:damg  0.638 -0.875 -0.735
```
]

.panel[.panel-name[Result-anova1]

```r
anova(mod1)
```

```
## Analysis of Variance Table
##              npar  Sum Sq Mean Sq F value
## light           1 251.887 251.887 29.1137
## damage          1 212.090 212.090 24.5138
## light:damage    1  49.802  49.802  5.7563
```
]

.panel[.panel-name[Result-anova2]

```r
library(lmerTest)
mod1 &lt;-lmer(growth~light*damage+(1+damage|shadehouse),damage)
anova(mod1)
```

```
## Type III Analysis of Variance Table with Satterthwaite's method
##               Sum Sq Mean Sq NumDF   DenDF F value    Pr(&gt;F)    
## light        202.281 202.281     1  8.5552 23.3801 0.0010710 ** 
## damage       194.623 194.623     1 10.7116 22.4949 0.0006526 ***
## light:damage  49.802  49.802     1 10.7116  5.7563 0.0358397 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]

.panel[.panel-name[Other-way1]

```r
#m0 without fixed effect
mod0 &lt;- update(mod1, ~.-light*damage)
library(pbkrtest)
KRmodcomp(mod0, mod1)
```

```
## large : growth ~ light * damage + (1 + damage | shadehouse)
## small : growth ~ (1 + damage | shadehouse)
##          stat     ndf     ddf F.scaling   p.value    
## Ftest 17.5396  3.0000  8.0424   0.88849 0.0006902 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]

.panel[.panel-name[Other-way2]

```r
library(car)
Anova(mod1, test='F')
```

```
## Analysis of Deviance Table (Type II Wald F tests with Kenward-Roger df)
## 
## Response: growth
##                    F Df Df.res   Pr(&gt;F)    
## light        31.9132  1 8.1631 0.000448 ***
## damage       24.4688  1 7.9150 0.001162 ** 
## light:damage  5.7266  1 8.1163 0.043222 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]
]
???
1. we put our all data into the matrix damage and set the variable of shafehouse and light as factors, load the package lme4 because of we need to use the function to build mixed model, so as you can see , this is also radom slope model,that means we assume the differnce between the growth caused by damge would be differnt in differnt shadehouse. and there is  interaction between damage and light, so here is multipy not plus.and then we summary this model
2. as you can see, we could not get more information from this 
1. so let's use  function of anova to Determine the importance of  each fixed effect in the total variation of response variable. here is two differnt anova function, the second one is from the package lmerTest. let's see what the difference.
3.  in the  first anova function see, no P value!!the same question just like the summary result! because when wo test mixed model, it could not caculte out the dgree freedom, so it would not provide the p value, but when we load package lMERtest, it could do that!
4. see p values are here, the results told us both fixed effect , light and damge are important even their interaction! in some situation , if their interaction is not significant, then we should rebuild the model, just change the multipy to plus and rerun the code, in some situation ,the damge term is not significant, but their interaction is significant, we could not remove the variable of damage.
5,6 here are other 2 function from differnt package  to test the fixed effect, the way1 compares the model we build and another model without the fixed part that means remove these two varibales. the result told us these two modles are totally differnt. the way2 is almost the same as the anova, but p value is a little bit differnt, because they use differnt ways to caculte the dgree freedom, anyway, this method Takes a more conservative approach to calculate, which means we would be easy to make the type two error,but on the other hand ,if we take the anova methond ,we would be easy  make the type one error. normally, It is more unacceptable for us to make type one error, especially in some medicine fields , because we can Judge valid as invalid (2),but we can't Judge invalid as valid(2),that would be dangerous right?
使用另两个包pbkrtest和car来估计固定因子的显著性，可以发现这两个包都是采取更保守的方法来估计，也就是手动计算部分的方法二，把随机效应的参数估计占了8个自由度.），则是将有效判断成无效（弃真）。在一定程度上，第一类错误和第二类错误是一枚硬币的两面。如果我们比较激进，则容易犯第一类错误；如果我们过于保守，第二类错误会主动找上门来。通常来说我们更不能接受的是第一类错误。
---
class: center, middle
# &lt;font color=red&gt;Q2: Is the random effect important ??? &lt;/font&gt;
???
so now is about evaluating the random effect
---
.panelset[

.panel[.panel-name[Code-Result]

```r
#mod_lm without random effect
mod_lm &lt;- lm(growth~light*damage, data=damage)
mod1 &lt;-lmer(growth~light*damage+(1+damage|shadehouse),damage)
#ML
anova(mod1, mod_lm)
#REML
anova(mod1, mod_lm,refit=FALSE)
library(lmerTest)
ranova(mod1)
library(MuMIn)
r.squaredGLMM(mod1)
library(MASS)
confint.result &lt;- confint(mod1, method='boot', oldNames=F, nsim = 99) 
confint.result
confint.result &lt;- confint(mod1, method='boot', oldNames=F, nsim = 2999) 
confint.result
```
]

.panel[.panel-name[Recall-ML]

.pull-left[
&lt;img src="E:/academic_resources/Tutorials/data/img/sta-slides1.png"
width="400" /&gt;
]
.pull-right[
### REML
&lt;font size=5&gt;In statistics, the restricted maximum likelihood (REML) approach is a particular form of maximum likelihood estimation that does not base estimates on a maximum likelihood fit of all the information, but instead uses a likelihood function calculated from a transformed set of data. In contrast to the earlier maximum likelihood estimation, REML can produce **unbiased estimates of variance** and covariance parameters&lt;/font&gt;
&lt;br&gt;
——Wikipedia
]
]

.panel[.panel-name[Result-anova1]

```
## refitting model(s) with ML (instead of REML)
```

```
## Data: damage
## Models:
## mod_lm: growth ~ light * damage
## mod1: growth ~ light * damage + (1 + damage | shadehouse)
##        npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  
## mod_lm    5 543.92 557.09 -266.96   533.92                       
## mod1      8 539.92 561.00 -261.96   523.92 9.9952  3    0.01861 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]

.panel[.panel-name[Result-anova2]

```
## Warning in anova.merMod(mod1, mod_lm, refit = FALSE): some models fit with REML
## = TRUE, some not
```

```
## Data: damage
## Models:
## mod_lm: growth ~ light * damage
## mod1: growth ~ light * damage + (1 + damage | shadehouse)
##        npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    
## mod_lm    5 543.92 557.09 -266.96   533.92                         
## mod1      8 528.03 549.11 -256.02   512.03 21.886  3   6.89e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]

.panel[.panel-name[Result-ranova]

```
## ANOVA-like table for random-effects: Single term deletions
## 
## Model:
## growth ~ light + damage + (1 + damage | shadehouse) + light:damage
##                                     npar  logLik    AIC   LRT Df Pr(&gt;Chisq)  
## &lt;none&gt;                                 8 -256.02 528.03                      
## damage in (1 + damage | shadehouse)    6 -259.46 530.91 6.881  2    0.03205 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

```
##            R2m       R2c
## [1,] 0.5637659 0.6607733
```
]

.panel[.panel-name[bootstrap]

```
##                                         2.5 %     97.5 %
## sd_(Intercept)|shadehouse           0.3185837  4.2636434
## cor_damage.(Intercept)|shadehouse  -1.0000000  0.9999902
## sd_damage|shadehouse                0.5646754 18.6213822
## sigma                               2.4317433  3.3317311
## (Intercept)                        -0.8006651  3.9510588
## lightL                              5.2392649 11.8344723
## damage                            -19.7773615  1.9961737
## lightL:damage                     -36.8002154 -5.5834274
```

```
##                                         2.5 %    97.5 %
## sd_(Intercept)|shadehouse           0.3770770  3.767717
## cor_damage.(Intercept)|shadehouse  -1.0000000  0.000000
## sd_damage|shadehouse                0.7008931 16.788227
## sigma                               2.4933295  3.337227
## (Intercept)                        -0.7646612  4.142013
## lightL                              4.8389538 11.498103
## damage                            -21.1858301  2.017946
## lightL:damage                     -34.9778276 -3.114681
```
]
]

???
2. when we evalute the radom effect, could we use anova just like what we did in the example 1? the answer is no, because when we use thay way which means we are using the maximum liklihood, in the linear mixed model we should use REML, and this is the biref introduction about REML from weikipedia, I'm not sure of the detailed math behind it. but REML just transform the data here, and use the transformed data to get the  unbiased parameter.let's see the code
1. in order to test the importance of random effect ,we need to built another model without random effect, and then compare them. when we use anova to compare them, 
3. there would be Tip Message which told us this function is using the ML method, 
1. but actually , in the anova function we can force it to use the REML method,just code  change the the refit to FALSE, 
4. but there would be Warning message,the ML method  was used  in linear model, the REML was used in linear mixed model, so actually they can't be compared
1. so the best way is to use the ranova from package lmertest,
5. see the result, the first term means the linear mixed model , the second term means remove the radom effect from linear mixed model,  and it would be just like the linear model we write before, thus using this function , we don't need to build another simpler model, and the result said the difference between these two model is signifcant , and  the model with random effect is better!!because its AIC is smaller!!! we also could use this function from package MUMIN, see the result, r squared m refers to the part explained by fixed effects, r aquared c refers to the part explained by random effects plus fixed effects, so the subtraction of the two is the true part explained by random effects. 
- and compare the p value we could find the p value in anova1 is too large and in anova2 is too small which means when we use the wrong method we are making the type 1 or 2 error!!!so anova couldn'be used everywhere!!!!!!
1. here is another way to evaluate the radom and fixed effect together, for every estimates it provides confidence interval instead of p value, some reference said the confidence interval is better than p value because  Confidence intervals quantify the uncertainty in the conclusion and provide more information than the p-value!! so we use the function confint() to get that, If we  want to test whether this predictor i put into the model is significantly different from zero, we can construct a 97.5% confidence interval to test whether this interval contains zero. In the process,  we  can additionally know how accurate my estimate is.  and If the confidence interval is too wide, then we need to collect more data. 
- by the way ,old Names =F means the result would show us  every full namen of predictor, when the oldnames is default, it is equal true which would only show the number 1,2,3, and the method we use to get confidence intervals is bootstrap, it is a sampling method with put-back, ,so the nism means the times of sampling. lets set different nism to compare.
6. see the  result , the intercept and slope of radom effect are both significantly different from 0, because their confidence interval not include 0 so we should put them into the model, and the lightL is significantly different from lightD, the intercept means lightD, so we also need to put this variable into our model, and the the CI include 0 , does this mean i should remove it???????NO!!because as we said before when their interaction is significant, both of them shoud't be removed even they are not signifcnat seperately. 
- this  is the result of sampling 99 times and this is  the result of sampling 99 times 2999 times, compare them ,we would find  The upper one has a wider confidence interval than the lower one!!!  as i said before when the  confidence interval is too wide, it is suggesting we need to  collect more data. thus when we can't increase the number of sampling in our experiments, bootstrap is a good way to incease it In a virtual way!!!

---
class: center, middle
# Thanks 
???
that' all, and i have to stress Some information is based on my personal understanding, so there would be some mistakes, maybe we could discuss now ,any question or comments???




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
